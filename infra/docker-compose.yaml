services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.1
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "2181"]
      interval: 10s
      timeout: 5s
      retries: 10


  kafka:
      image: confluentinc/cp-kafka:7.6.1
      depends_on:
        zookeeper:
          condition: service_healthy
      ports:
        - "9092:9092"    # container internal
        - "29092:29092"  # host-accessible
      environment:
        KAFKA_BROKER_ID: 1
        KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
        KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
        KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092
        KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
        KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
        KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
        KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
        KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
        KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      volumes:
        - ./kafka:/data/kafka 
      healthcheck:
        test: ["CMD-SHELL", "kafka-broker-api-versions --bootstrap-server localhost:9092"]
        interval: 10s
        timeout: 5s
        retries: 10

  kafka-topic-init:
      image: confluentinc/cp-kafka:7.6.1
      depends_on:
        kafka:
          condition: service_healthy
      entrypoint: [ "/bin/bash", "-c", "/scripts/create_topics.sh" ]
      volumes:
        - ../scripts:/scripts

  
  # New initialization container for Spark permissions
  spark-init:
    image: apache/spark:3.4.2
    container_name: spark-init
    volumes:
      - spark_ivy_cache:/home/spark/.ivy2
    command: bash -c "chown -R spark:spark /home/spark/.ivy2"
    user: root
    # Run and exit before starting Spark
    depends_on:
      - kafka

  spark:
    image: apache/spark:3.4.2
    container_name: spark
    volumes:
      - ../streaming:/opt/workspace/streaming
      - spark_ivy_cache:/home/spark/.ivy2
    working_dir: /opt/workspace
    # IMPORTANT: Explicitly set the user to 'spark' to match the directory ownership
    user: spark
    # command: >
    #   /opt/spark/bin/spark-submit
    #   --master local[*]
    #   --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1
    #   /opt/workspace/streaming/spark_app/stream_processor.py
    command: >
      /opt/spark/bin/spark-submit
      --master local[*]
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,org.mongodb.spark:mongo-spark-connector_2.12:10.2.0
      /opt/workspace/streaming/spark_app/stream_processor_mongo.py
    depends_on:
      - kafka
      - spark-init # Ensure initialization runs first
      - kafka-topic-init
    environment:
      SPARK_NO_DAEMONIZE: "true"

  grafana:
    image: grafana/grafana:latest
    ports: ["3000:3000"]
    volumes:
      - ./grafana:/etc/grafana/provisioning

  mongodb:
    image: mongo:6
    ports: ["27017:27017"]
    volumes:
      - mongo_data:/data/db

volumes:
  mongo_data:
  spark_ivy_cache:
